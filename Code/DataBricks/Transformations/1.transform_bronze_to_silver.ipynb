{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9212ffc9-d907-4c7c-9415-20e68c8943db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Load data from bronze layer to silver Layer\n",
    "#### Steps\n",
    "1. Run the configurations file which has the folder paths defined.\n",
    "2. Load into silver layer after casting datetime columns to date columns and dropping the rowguid column from all tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59a02b68-a8dd-45c5-b992-8e7f2807c71e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../SetUp/Configuration\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5007978-ea6e-47c8-915c-ed61987e2b49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0334e4c-a197-4be8-af3b-3be4c5376557",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import col, lit, date_format , from_utc_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "134a3d05-a80b-4012-ac39-879a93cbdffa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Populate the table names from bronze layer in a list \n",
    "table_names=[]\n",
    "for table in dbutils.fs.ls(f'{bronze_folder_path}/SalesLT/'):\n",
    "    table_names.append(table.name.split('/')[0])\n",
    "\n",
    "for table in table_names:\n",
    "    df = spark.read \\\n",
    "    .option(\"header\", True)\\\n",
    "    .parquet(f'{bronze_folder_path}/SalesLT/{table}/{table}.parquet')\n",
    "    \n",
    "    # convert the datetime columns to date columns\n",
    "    # Dropping the rowguid column from all tables\n",
    "    for col in df.columns:\n",
    "        if \"Date\" in col or \"date\" in col :\n",
    "            df= df.withColumn(col,date_format(from_utc_timestamp(df[col].\n",
    "            cast(TimestampType()),\"UTC\"),\"yyyy-MM-dd\"))\n",
    "        \n",
    "        if col == 'rowguid' :\n",
    "            df = df.drop(col)\n",
    "            \n",
    "    # write the data to silver layer in delta format\n",
    "    output_path = f'{silver_folder_path}/SalesLT/{table}/'\n",
    "    df.write.format('delta').mode(\"overwrite\").save(output_path)        "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1.transform_bronze_to_silver",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
